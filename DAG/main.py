import asyncio
import re
from typing import List

from sqlalchemy import String, select, text
from sqlalchemy.ext.asyncio import (
    AsyncAttrs,
    async_sessionmaker,
    create_async_engine,
    AsyncSession
)
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from pgvector.sqlalchemy import Vector

# æ–°å¢ï¼šNLP ç›¸å…³çš„åº“
import jieba
from sentence_transformers import SentenceTransformer

# 1. é…ç½®éƒ¨åˆ†
DATABASE_URL = "postgresql+asyncpg://mlops_user:mlops_password@localhost:5432/mlops_db"
# é€‰æ‹©ä¸€ä¸ªå¯¹ä¸­æ–‡æ”¯æŒå¾ˆå¥½çš„è½»é‡çº§æ¨¡å‹ (ç»´åº¦é€šå¸¸æ˜¯ 768)
MODEL_NAME = 'shibing624/text2vec-base-chinese' 

# 2. NLP å¤„ç†æ¨¡å— (æ¨¡æ‹Ÿ MLOps ä¸­çš„æ¨¡å‹æœåŠ¡)
class ChineseNLPProcessor:
    def __init__(self):
        print(f"â³ Loading model '{MODEL_NAME}'... (might take a while first time)")
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model = SentenceTransformer(MODEL_NAME)
        print("âœ… Model loaded.")

    def clean_text(self, text: str) -> str:
        """åŸºç¡€é¢„å¤„ç†ï¼šå»é™¤ç‰¹æ®Šç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡ã€æ•°å­—å’Œè‹±æ–‡"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ ‡ç‚¹ç¬¦å·ç­‰
        text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", text)
        return text

    def get_embedding(self, text: str) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        # 1. ç®€å•æ¸…æ´—
        cleaned_text = self.clean_text(text)
        # 2. (å¯é€‰) è™½ç„¶ BERT ç±»æ¨¡å‹ä¸éœ€è¦ jieba åˆ†è¯ï¼Œä½†åœ¨ä¼ ç»Ÿ NLP ä¸­å¸¸ç”¨äºå…³é”®è¯æå–
        # è¿™é‡Œä¸ºäº†æ¼”ç¤º jieba çš„é›†æˆï¼š
        words = jieba.lcut(cleaned_text) 
        print(f"   [Preprocess] Segments: {words}")
        
        # 3. ç”Ÿæˆå‘é‡ (è¿™æ˜¯ CPU å¯†é›†å‹æ“ä½œï¼)
        embedding = self.model.encode(cleaned_text)
        return embedding.tolist()

# åˆå§‹åŒ–å…¨å±€ NLP å¤„ç†å™¨
nlp_processor = None

# 3. æ•°æ®åº“æ¨¡å‹å®šä¹‰
engine = create_async_engine(DATABASE_URL, echo=False)
AsyncSessionLocal = async_sessionmaker(bind=engine, expire_on_commit=False)

class Base(AsyncAttrs, DeclarativeBase):
    pass

class KnowledgeBase(Base):
    __tablename__ = "knowledge_base"

    id: Mapped[int] = mapped_column(primary_key=True)
    raw_content: Mapped[str] = mapped_column(String(1024)) # åŸå§‹æ–‡æœ¬
    
    # !æ³¨æ„!ï¼štext2vec-base-chinese è¾“å‡ºç»´åº¦æ˜¯ 768
    embedding: Mapped[List[float]] = mapped_column(Vector(768)) 

    def __repr__(self):
        return f"<KB(id={self.id}, content='{self.raw_content[:20]}...')>"

# 4. æ ¸å¿ƒå¼‚æ­¥é€»è¾‘

async def init_db():
    async with engine.begin() as conn:
        await conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)

async def add_document(session: AsyncSession, text_content: str):
    """
    å¼‚æ­¥æ·»åŠ æ–‡æ¡£ï¼š
    å…³é”®ç‚¹ï¼šæ¨¡å‹è®¡ç®—æ˜¯ CPU å¯†é›†å‹çš„ï¼Œä¸èƒ½ç›´æ¥é˜»å¡å¼‚æ­¥å¾ªç¯ã€‚
    """
    print(f"\nâ• Adding: {text_content}")
    
    # !é‡è¦æŠ€å·§!ï¼šä½¿ç”¨ asyncio.to_thread å°† CPU å¯†é›†çš„å‘é‡åŒ–æ“ä½œ
    # æ‰”åˆ°å¦ä¸€ä¸ªçº¿ç¨‹å»è·‘ï¼Œé˜²æ­¢å¡ä½æ•´ä¸ªç¨‹åºçš„ Event Loop
    vector = await asyncio.to_thread(nlp_processor.get_embedding, text_content)
    
    doc = KnowledgeBase(raw_content=text_content, embedding=vector)
    session.add(doc)
    await session.commit()
    print("   âœ… Saved to DB.")

async def search_similar(session: AsyncSession, query_text: str, limit: int = 2):
    print(f"\nğŸ” Query: '{query_text}'")
    
    # 1. åŒæ ·æŠŠæŸ¥è¯¢æ–‡æœ¬çš„å‘é‡åŒ–è¿‡ç¨‹æ”¾åˆ°çº¿ç¨‹æ± 
    query_vec = await asyncio.to_thread(nlp_processor.get_embedding, query_text)
    
    # 2. æ•°æ®åº“æŸ¥è¯¢ (IO å¯†é›†å‹ï¼Œä½¿ç”¨ await)
    stmt = select(KnowledgeBase).order_by(
        KnowledgeBase.embedding.l2_distance(query_vec)
    ).limit(limit)
    
    result = await session.execute(stmt)
    hits = result.scalars().all()
    
    print("   â¬‡ï¸ Results:")
    for hit in hits:
        # è®¡ç®—è·ç¦»é€šå¸¸ä¹Ÿå¯ä»¥åœ¨ Python ç®—ï¼Œä½†è¿™é‡Œæ•°æ®åº“å·²ç»æ’å¥½åºäº†
        print(f"   ğŸ“„ {hit.raw_content}")

async def main():
    global nlp_processor
    # åœ¨ä¸»ç¨‹åºå¼€å§‹æ—¶åŠ è½½æ¨¡å‹
    nlp_processor = ChineseNLPProcessor()
    
    await init_db()

    async with AsyncSessionLocal() as session:
        # 1. å‡†å¤‡ä¸€äº›ä¸­æ–‡è¯­æ–™
        corpus = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œä¸“æ³¨äºåˆ©ç”¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
            "Pythonæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œéå¸¸é€‚åˆæ•°æ®ç§‘å­¦ã€‚",
            "è¥¿çº¢æŸ¿ç‚’é¸¡è›‹æ˜¯ä¸€é“éå¸¸å—æ¬¢è¿çš„ä¸­å›½å®¶å¸¸èœã€‚",
            "å¦‚ä½•çƒ¹é¥ªç¾å‘³çš„ç‰›æ’ï¼Ÿéœ€è¦æ§åˆ¶å¥½ç«å€™ã€‚",
        ]

        # 2. æ’å…¥æ•°æ®
        for text in corpus:
            await add_document(session, text)
        
        # 3. è¯­ä¹‰æœç´¢æµ‹è¯•
        # æ¡ˆä¾‹ A: æœæŠ€æœ¯ç›¸å…³
        await search_similar(session, "AIå’Œç¥ç»ç½‘ç»œçš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ")
        
        # æ¡ˆä¾‹ B: æœé£Ÿç‰©ç›¸å…³
        await search_similar(session, "è‚šå­é¥¿äº†åƒä»€ä¹ˆèœå¥½ï¼Ÿ")

    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main())